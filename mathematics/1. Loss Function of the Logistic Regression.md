## On the Loss Function of the Logistic Regression

In machine learning mathematics, it is standard to see the following equation for the loss of the logistic regression:
  
```math
(1)\quad
L(\hat{y}, y) = -\left( y \log(\hat{y}) + (1 - y)\log(1 - \hat{y}) \right)
```

  
It is easy understand that it is simply the logarithm of the function $P(y \mid x) = \hat{y}^{y} (1 - \hat{y})^{1 - y}$, given $P(y=1 \mid x) = \hat{y}$, with which we are taking in order to simplify proceeding calculus needed for training.  

Where did the equation for $P(y \mid x)$ come from ?  
  
One frequent explanation states that this comes from the _Bernoulli conditional probability_ of which variables predicted through standard logistic regression satisfy.  

Another explains that this is simply a compact form of the following piecewise function:  

```math
(2)\quad
P(y \mid x) =
\begin{cases}
\hat{y} & \text{if } y = 1 \\
1 - \hat{y} & \text{if } y = 0
\end{cases}
```
Intuitively, $\hat{y}^{y} (1 - \hat{y})^{1 - y}$ really does implement (2). Simpler in explanation, yet not without magic. Why not have something simpler like:  

```math
(3)\quad
\hat{y}y + (1 - \hat{y})(1 - y)
```  
  
The explanation rests on the fact that while both of them validly implement (2), the former one - $\hat{y}^{y} (1 - \hat{y})^{1 - y}$ is more convenient mathematically for succeeding steps. In fact, (3) is also known as an _additive indicator form of the Bernoulli probability_ and is not technically invalid, just useless in our objectives.  

What makes the problem flexible in our case to choose out of convenience is that it is only defined in two y values - $y \in \{0,1\}$. Whatever happens in between, or outside of these two values 1 and 0, is undefined so as long as our function satisfies our constraints in (1).

---

While it is intuitive to see that (2) satisfies (1), a more creative way of showing this is as follows:
```math
(4)\quad
\hat{y}^{y} =
\begin{cases}
\hat{y} & \text{if } y = 1 \\
1 & \text{if } y = 0
\end{cases}
```

```math
(5)\quad
(1 - \hat{y})^{1 - y} =
\begin{cases}
1 & \text{if } y = 1 \\
1 - \hat{y} & \text{if } y = 0
\end{cases}
```
(4) and (5) comes from simple expansion.  

Multiplying both piecewise functions (4) and (5) results to:

```math
(6)\quad
\hat{y}^{y} (1 - \hat{y})^{1 - y} =
\begin{cases}
\hat{y} & \text{if } y = 1 \\
1 - \hat{y} & \text{if } y = 0
\end{cases}
```

Combining (6) and (2) results to what we are trying to prove.

```math
(7)\quad
P(y \mid x) = \hat{y}^{y} (1 - \hat{y})^{1 - y}
```



